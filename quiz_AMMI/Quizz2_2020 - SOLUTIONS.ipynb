{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2\n",
    "*AMMI 2020, May 29th*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Section 1 - Kernel mathematics\n",
    "You are encouraged to use the notation `s(k, x, a)` $ = \\sum_{i, j} a_i a_j k(x_i, x_j)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "What are the conditions for $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ to be a positive definite (p.d.) kernel?\n",
    "\n",
    "#### Solution a\n",
    "1. $k$ must be symmetric $k(x, y) = k(y, x)$\n",
    "2. For all `x` and `a`: `s(k, x, a)`$\\geq 0$ \n",
    "\n",
    "#### Solution b\n",
    "1. symmetric\n",
    "2. For every `x` the Gram-matrix $K_{ij} = k(x_i, x_j)$ is positive semi-definite\n",
    "\n",
    "#### Solution c\n",
    "There exists a Hilbert space $\\mathcal{H}$ and a mapping $\\Phi: \\mathcal{X} \\to \\mathcal{H}$ such that $k(x, y) = <\\Phi(x), \\Phi(y)>_\\mathcal{H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Let $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ be a p.d. kernel, and $f: \\mathcal{Y} \\to \\mathcal{X}$ a function.\n",
    "\n",
    "We define a new function $k': \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}$ by composing $f$ and $k$: $$k'(x, y) = k\\left(f(x),\\, f(y)\\right)$$\n",
    "Show that $k'$ is a p.d. kernel\n",
    "\n",
    "#### Solution\n",
    "\n",
    "1. $k'(x, y) = k(f(x), f(y)) = k(f(y), f(x)) = k'(y, x)$\n",
    " \n",
    " (Second equality is because $k$ is symmetric)\n",
    "\n",
    " So $k'$ is symmetric.\n",
    " \n",
    "2. Let $x_i \\in \\mathcal{X}$, $a_i \\in \\mathbb{R}$\n",
    "\n",
    " `s(k',x,a) = s(k,f(x),a) `$\\geq 0$ because $k$ is p.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Instructions for rest of section:\n",
    "\n",
    "For each of these functions $k:\\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$, state if they are p.d. or not. Prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "#### (a) $\\mathcal{X} = \\mathbb{R}$, $\\, \\,\\quad k_1(x, y) = xy$\n",
    "\n",
    "    YES\n",
    " - symmetric: OK\n",
    " - `s(k1,x,a)` $= \\left(\\sum_i a_ix_i\\right)^2 \\geq 0$\n",
    "\n",
    "#### (b) $\\mathcal{X} = \\mathbb{R^*}$, $\\quad k_2(x, y) = \\frac{1}{xy}$\n",
    "\n",
    "    YES\n",
    " - Direct application of question 2 with $f(x) = \\frac{1}{x}$ and $k=k_1$\n",
    " \n",
    "#### (c) $\\mathcal{X} = \\mathbb{R}$, $\\,\\,\\quad k_3(x, y) = \\min (x, y)$\n",
    "*We know from the course that $k$ is p.d. for non-negative numbers ($\\mathcal{X} = \\mathbb{R_+}$). <br>Is this true when we extend to all real numbers?*\n",
    "\n",
    "    NO\n",
    " - Choose $x = -1$ and $a = 1$: \n",
    " \n",
    "  $s(k_3, x, a) = a^2 \\min(-1, -1) = -1 < 0$\n",
    "  \n",
    "*Remark: With this same proof, we have a necessary (not sufficient) condition for $k$ to be p.d.: $k(x,x)$ must be $\\geq 0$ for all $x$*\n",
    "\n",
    "#### (d) $\\mathcal{X} = \\mathbb{R_+}$, $\\quad k_4(x, y) = \\max(x, y)$\n",
    "\n",
    "    NO\n",
    " - Choose $x_1 < x_2$ and $a_1 = 1$, $a_2 = -1$: \n",
    " \n",
    "  $s(k_4, x, a) = x_1^2 - x_2^2 < 0$\n",
    "  \n",
    "#### (e)$\\mathcal{X} = \\mathbb{R_+}$, $\\quad k_5(x, y) = \\dfrac{\\min(x, y)}{\\max (x, y)}$\n",
    "*Hint: you can use the questions above, and the fact that the product of p.d. kernels is also p.d.*\n",
    "\n",
    "    YES\n",
    "- $k_5(x, y) = \\dfrac{\\min^2(x, y)}{\\max (x, y) \\min (x, y)}$\n",
    " \n",
    " Now since $\\max (x, y) \\min (x, y) = xy$, we notice that\n",
    " $k_5 = k_\\min^2 k_2$, which is p.d. as the product of p.d. kernels\n",
    " \n",
    "- Other proof: $k_6(x, y) = \\frac{1}{\\max(x,y)} = \\min(\\frac{1}{x}, \\frac{1}{y})$ is pd by application of question 2. with $f(x)=\\frac{1}{x}$ and $k= k_\\min$\n",
    "\n",
    " $k_5 = k_\\min k_6$ is p.d. as the product of p.d. kernels\n",
    " \n",
    "#### (f) $\\mathcal{X} =  \\left\\{\\text{Subsets of }\\Omega\\right\\}$ where $\\Omega$ is a fixed set, $\\quad k(A, B) = \\mathbb{P}(A\\cap B)$\n",
    "\n",
    "*Hint: Write the probability as an expectation*\n",
    "\n",
    "    YES\n",
    "- It is symmetric\n",
    "- We use the identity $\\mathbb{P}(A\\cap B) = \\mathbb{E}(\\mathbb{1}_A\\mathbb{1}_B)$\n",
    " \n",
    " $$\\begin{aligned}\n",
    " \\sum_{i, j} a_i a_j k(A_i, A_j) &= \\mathbb{E}\\left[\\sum_{i, j} a_i a_j \\mathbb{1}_{A_i}\\mathbb{1}_{A_j}\\right]\\\\\n",
    " &= \\mathbb{E}\\left[\\left(\\sum_{i} a_i \\mathbb{1}_{A_i}\\right)^2\\right]\\geq 0\n",
    " \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Why does the RKHS of  $k(x, y) = (x y)^2$ contains quadratic functions? Can it contain functions that are not quadratic? \n",
    "\n",
    "#### Solution\n",
    "The RKHS $\\mathcal{H}$ of $k(x, y) = (x y)^2$ contains all functions of the form $x \\rightarrow k(t,x) = t^2 x^2$ (these functions are denoted by $K_t$ in the course). It also contains all linear combinations of these functions, so if we denote $q_a(x) := a x^2$ and\n",
    "$$\\mathcal{H}_0 := \\left\\{ q_a ; a\\in \\mathbb{R}\\right\\}$$ the set of quadratic functions, then we can write\n",
    "$$\\mathcal{H}_0 \\subset \\mathcal{H}$$\n",
    "\n",
    "It is straightforward to prove that $\\mathcal{H}_0$ is complete, so\n",
    "$$\\mathcal{H}_0 = \\mathcal{H}$$\n",
    "\n",
    "Note: $\\langle q_a, q_b\\rangle_{\\mathcal{H}} = sign(a)sign(b)\\sqrt{\\mid ab \\mid}$\n",
    "\n",
    "Note 2: I saw some confusion in the answers. In general, $\\Phi(x)$ is NOT an element of the RKHS (***elements of the RKHS are functions***). It does however map to an element of $\\mathcal{H}$ which is the function $t \\rightarrow \\Phi(x)^\\top \\Phi(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Section 2 - Practical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "1. Explain the kernel trick.\n",
    "2. What are its advantages?\n",
    "3. What conditions must the model verify to apply the kernel trick?\n",
    "\n",
    "#### Solution\n",
    "3) The kernel trick applies to models which depend, in their optimization and in their decision function, only on scalar products between the data points x.\n",
    "\n",
    "1) It consists in replacing these scalar products $x_i^\\top x_j$ by a kernel function $k(x_i, x_j) = \\Phi(x_i)^\\top\\Phi(x_j)$. This corresponds to implicitly mapping the data points to another feature space.\n",
    "\n",
    "2) The advantages are twofold:\n",
    " - A linear model can be used to learn a nonlinear function of the input, by mapping it to a higher (potentially infinite) dimensional feature space. This makes for richer and more sophisticated models while keeping the optimization simple\n",
    " - The complexity of the optimization only depends on the kernel Gram matrix, not on the features directly. It is therefore useful in cases where the number of features is larger than the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Consider kernel SVM with kernel function $K$. Suppose you have the solution $\\alpha$ to the dual problem.\n",
    "\n",
    "How do you compute the bias $b$? Use only the kernel matrix $K$ in your answer\n",
    "\n",
    "#### Solution\n",
    "Seen in practical session 6\n",
    "\n",
    "In linear SVM:\n",
    "$$b = y_{i^*} - \\sum_j \\alpha_j y_j x_j^\\top x_{i^*} \\quad \\textit{for support vectors }i^*$$\n",
    "\n",
    "Hence in SVM with kernel function $k$:\n",
    "$$b = y_{i^*} - \\sum_j \\alpha_j y_j k(x_j, x_{i^*})$$\n",
    "\n",
    "Expressed in matrix notation:\n",
    "$$b = y_{i^*} - K_{i^*}^\\top \\left(\\alpha \\odot y\\right)$$\n",
    "where $\\odot$ denotes elementwise multiplication.\n",
    "\n",
    "Note: The support vectors are given by the condition $0<\\alpha_{i^*}<C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "What is the effect of $\\sigma^2$ on the RKHS of the RBF kernel?  Does it have a regularization effect?\n",
    "\n",
    "#### Solution\n",
    "As seen in slide 188, the norm of an element $f$ of the RKHS is given by:\n",
    "$$\\|f\\|_\\mathcal{H}^2 = \\int\\lvert\\hat{f}(\\omega)\\rvert^2e^{\\frac{\\sigma^2\\omega^2}{2}}d\\omega < +\\infty$$\n",
    "Therefore as $\\sigma^2$ increases, $\\hat{f}$ must be more and more concentrated near 0 for the integral to stay finite. Increasing $\\sigma^2$ acts as a soft frequency filter: the high frequencies of $f$ are flattened.\n",
    "\n",
    "This has a regularization effect as the function $f$ becomes smoother with less variations (seen in practical session 6).\n",
    "\n",
    "Intuitively: for very large $\\sigma$, $k$ is constant equal to 1, so a very regular function. On the other hand for very small $\\sigma$, $k(x,y)=\\mathbb{1}_{x=y}$, which leads certainly to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "We have seen that a trick to implement the spectrum kernel consists of representing all possible subsequences of size $k$ by an integer and using a sparse vector structure. What are the limitations of this approach? Can we do it for all values of $k$?\n",
    "\n",
    "#### Solution\n",
    "Expected answer: The indices of $k$-mers are coded onto maximum 64 bits, hence $k$ cannot be larger than 32 ($4^{32} = 2^{64}$).\n",
    "For $k$-mers larger than 32 one could use a hashing function, but this would imply some collisions.\n",
    "\n",
    "Points were given for all answers that made some sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
